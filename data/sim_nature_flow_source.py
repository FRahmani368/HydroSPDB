""" data source for simulated flow generated by model trained by dataset of ref basins"""
import os
import pandas as pd
import numpy as np
import torch

from data import GagesConfig, GagesSource, DataModel
from explore import trans_norm
from hydroDL import master_train
from hydroDL.model import model_run
from utils import serialize_numpy, serialize_pickle, serialize_json, unserialize_pickle, unserialize_json, \
    unserialize_numpy


class SimNatureFlowSource(object):
    def __init__(self, t_range, config_data, sim_config_file, subdir=None, *args):
        self.t_range = t_range
        if len(args) == 0:
            if subdir:
                sim_config_data = GagesConfig.set_subdir(sim_config_file, subdir)
            else:
                sim_config_data = GagesConfig(sim_config_file)
            sim_source_data = GagesSource(sim_config_data, t_range)
            source_data = GagesSource(config_data, t_range)
            self.sim_model_data = DataModel(sim_source_data)
            self.model_data = DataModel(source_data)

        else:
            self.sim_model_data = args[0]
            self.model_data = args[1]

    def write_temp_source(self, is_test=False):
        """wirte source object to some temp files"""

        def save_datamodel(data_model, num_str, **kwargs):
            dir_temp = os.path.join(data_model.data_source.data_config.data_path["Temp"], num_str)
            if not os.path.isdir(dir_temp):
                os.makedirs(dir_temp)
            data_source_file = os.path.join(dir_temp, kwargs['data_source_file_name'])
            stat_file = os.path.join(dir_temp, kwargs['stat_file_name'])
            flow_file = os.path.join(dir_temp, kwargs['flow_file_name'])
            forcing_file = os.path.join(dir_temp, kwargs['forcing_file_name'])
            attr_file = os.path.join(dir_temp, kwargs['attr_file_name'])
            f_dict_file = os.path.join(dir_temp, kwargs['f_dict_file_name'])
            var_dict_file = os.path.join(dir_temp, kwargs['var_dict_file_name'])
            t_s_dict_file = os.path.join(dir_temp, kwargs['t_s_dict_file_name'])
            serialize_pickle(data_model.data_source, data_source_file)
            serialize_json(data_model.stat_dict, stat_file)
            serialize_numpy(data_model.data_flow, flow_file)
            serialize_numpy(data_model.data_forcing, forcing_file)
            serialize_numpy(data_model.data_attr, attr_file)
            # dictFactorize.json is the explanation of value of categorical variables
            serialize_json(data_model.f_dict, f_dict_file)
            serialize_json(data_model.var_dict, var_dict_file)
            serialize_json(data_model.t_s_dict, t_s_dict_file)

        if is_test:
            save_datamodel(self.sim_model_data, "1", data_source_file_name='test_data_source.txt',
                           stat_file_name='test_Statistics.json', flow_file_name='test_flow',
                           forcing_file_name='test_forcing', attr_file_name='test_attr',
                           f_dict_file_name='test_dictFactorize.json', var_dict_file_name='test_dictAttribute.json',
                           t_s_dict_file_name='test_dictTimeSpace.json')
            save_datamodel(self.model_data, "2", data_source_file_name='test_data_source.txt',
                           stat_file_name='test_Statistics.json', flow_file_name='test_flow',
                           forcing_file_name='test_forcing', attr_file_name='test_attr',
                           f_dict_file_name='test_dictFactorize.json', var_dict_file_name='test_dictAttribute.json',
                           t_s_dict_file_name='test_dictTimeSpace.json')
        else:
            save_datamodel(self.sim_model_data, "1", data_source_file_name='data_source.txt',
                           stat_file_name='Statistics.json', flow_file_name='flow', forcing_file_name='forcing',
                           attr_file_name='attr', f_dict_file_name='dictFactorize.json',
                           var_dict_file_name='dictAttribute.json', t_s_dict_file_name='dictTimeSpace.json')
            save_datamodel(self.model_data, "2", data_source_file_name='data_source.txt',
                           stat_file_name='Statistics.json', flow_file_name='flow', forcing_file_name='forcing',
                           attr_file_name='attr', f_dict_file_name='dictFactorize.json',
                           var_dict_file_name='dictAttribute.json', t_s_dict_file_name='dictTimeSpace.json')

    @classmethod
    def get_sim_nature_flow_source(cls, config_data, t_range, sim_config_file, subdir=None, is_test=False):
        def load_datamodel(dir_temp_orgin, num_str, **kwargs):
            dir_temp = os.path.join(dir_temp_orgin, num_str)
            data_source_file = os.path.join(dir_temp, kwargs['data_source_file_name'])
            stat_file = os.path.join(dir_temp, kwargs['stat_file_name'])
            flow_npy_file = os.path.join(dir_temp, kwargs['flow_file_name'])
            forcing_npy_file = os.path.join(dir_temp, kwargs['forcing_file_name'])
            attr_npy_file = os.path.join(dir_temp, kwargs['attr_file_name'])
            f_dict_file = os.path.join(dir_temp, kwargs['f_dict_file_name'])
            var_dict_file = os.path.join(dir_temp, kwargs['var_dict_file_name'])
            t_s_dict_file = os.path.join(dir_temp, kwargs['t_s_dict_file_name'])
            source_data = unserialize_pickle(data_source_file)
            # 存储data_model，因为data_model里的数据如果直接序列化会比较慢，所以各部分分别序列化，dict的直接序列化为json文件，数据的HDF5
            stat_dict = unserialize_json(stat_file)
            data_flow = unserialize_numpy(flow_npy_file)
            data_forcing = unserialize_numpy(forcing_npy_file)
            data_attr = unserialize_numpy(attr_npy_file)
            # dictFactorize.json is the explanation of value of categorical variables
            var_dict = unserialize_json(var_dict_file)
            f_dict = unserialize_json(f_dict_file)
            t_s_dict = unserialize_json(t_s_dict_file)
            data_model = DataModel(source_data, data_flow, data_forcing, data_attr, var_dict, f_dict, stat_dict,
                                   t_s_dict)
            return data_model

        temp_dir = config_data.data_path["Temp"]
        if is_test:
            sim_model_data = load_datamodel(temp_dir, "1", data_source_file_name='test_data_source.txt',
                                            stat_file_name='test_Statistics.json', flow_file_name='test_flow.npy',
                                            forcing_file_name='test_forcing.npy', attr_file_name='test_attr.npy',
                                            f_dict_file_name='test_dictFactorize.json',
                                            var_dict_file_name='test_dictAttribute.json',
                                            t_s_dict_file_name='test_dictTimeSpace.json')
            model_data = load_datamodel(temp_dir, "2", data_source_file_name='test_data_source.txt',
                                        stat_file_name='test_Statistics.json', flow_file_name='test_flow.npy',
                                        forcing_file_name='test_forcing.npy', attr_file_name='test_attr.npy',
                                        f_dict_file_name='test_dictFactorize.json',
                                        var_dict_file_name='test_dictAttribute.json',
                                        t_s_dict_file_name='test_dictTimeSpace.json')
        else:
            sim_model_data = load_datamodel(temp_dir, "1", data_source_file_name='data_source.txt',
                                            stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                            forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                            f_dict_file_name='dictFactorize.json',
                                            var_dict_file_name='dictAttribute.json',
                                            t_s_dict_file_name='dictTimeSpace.json')
            model_data = load_datamodel(temp_dir, "2", data_source_file_name='data_source.txt',
                                        stat_file_name='Statistics.json', flow_file_name='flow.npy',
                                        forcing_file_name='forcing.npy', attr_file_name='attr.npy',
                                        f_dict_file_name='dictFactorize.json',
                                        var_dict_file_name='dictAttribute.json',
                                        t_s_dict_file_name='dictTimeSpace.json')

        sim_nature_flow_source = cls(t_range, config_data, sim_config_file, subdir, sim_model_data, model_data)
        return sim_nature_flow_source

    def read_natural_inflow(self):
        sim_model_data = self.sim_model_data
        sim_config_data = sim_model_data.data_source.data_config
        # read model
        # firstly, check if the model used to generate natural flow has existed
        out_folder = sim_config_data.data_path["Out"]
        epoch = sim_config_data.model_dict["train"]["nEpoch"]
        model_file = os.path.join(out_folder, 'model_Ep' + str(epoch) + '.pt')
        if not os.path.isfile(model_file):
            master_train(sim_model_data)
        model = torch.load(model_file)
        # run the model
        batch_size = sim_config_data.model_dict["train"]["miniBatch"][0]
        model_data = self.model_data
        config_data = model_data.data_source.data_config
        model_dict = config_data.model_dict
        x, y, c = model_data.load_data(model_dict)
        t_range = self.t_range
        epoch = model_dict["train"]["nEpoch"]
        file_name = '_'.join([str(t_range[0]), str(t_range[1]), 'ep' + str(epoch)])
        file_path = os.path.join(out_folder, file_name) + '.csv'
        model_run.model_test(model, x, c, file_path=file_path, batch_size=batch_size)
        # read natural_flow from file
        np_natural_flow = pd.read_csv(file_path, dtype=np.float, header=None).values
        return np_natural_flow

    def prepare_flow_data(self):
        """generate flow from model, reshape to a 3d array, and transform to tensor:
        1d: nx * ny_per_nx
        2d: miniBatch[1]
        3d: length of time sequence, now also miniBatch[1]
        """
        # read data for model of allref
        sim_model_data = self.sim_model_data
        sim_config_data = sim_model_data.data_source.data_config
        batch_size = sim_config_data.model_dict["train"]["miniBatch"][0]
        x, y, c = sim_model_data.load_data(sim_config_data.model_dict)
        # read model
        out_folder = self.data_config.data_path["Out"]
        epoch = sim_config_data.model_dict["train"]["nEpoch"]
        model = model_run.model_load(out_folder, epoch, model_name='model')
        # run the model
        model_dict = self.data_config.model_dict
        t_range = self.t_range
        epoch = model_dict["train"]["nEpoch"]
        file_name = '_'.join([str(t_range[0]), str(t_range[1]), 'ep' + str(epoch)])
        file_path = os.path.join(out_folder, file_name) + '.csv'
        model_run.model_test(model, x, c, file_path=file_path, batch_size=batch_size)
        # read natural_flow from file
        np_natural_flow = pd.read_csv(file_path, dtype=np.float, header=None).values
        nx = np_natural_flow.shape[0]
        all_time_length = np_natural_flow.shape[1]
        np_natural_flow_2d = np_natural_flow.reshape(nx, all_time_length)
        rho = self.data_config.model_dict["train"]["miniBatch"][1]
        x_np = np.zeros([nx, all_time_length - rho + 1, rho])
        for i in range(nx):
            for j in range(all_time_length - rho + 1):
                x_np[i, j, :] = np_natural_flow_2d[i][j:j + rho]
        ny_per_nx = x_np.shape[1] - rho + 1
        # TODO: this method has something wrong, memory not enough
        x_tensor = torch.zeros([nx * ny_per_nx, rho, rho], requires_grad=False)
        for i in range(nx):
            per_x_np = np.zeros([ny_per_nx, rho, rho])
            for j in range(ny_per_nx):
                per_x_np[j, :, :] = x_np[i, j:j + rho, :]
            x_tensor[(i * ny_per_nx):((i + 1) * ny_per_nx), :, :] = torch.from_numpy(per_x_np)
        print("streamflow data Ready! ...")
        return x_tensor

    def read_outflow(self):
        """read streamflow data as observation data, transform array to tensor"""
        sim_model_data = self.sim_model_data
        data_flow = sim_model_data.data_flow
        data = np.expand_dims(data_flow, axis=2)
        stat_dict = sim_model_data.stat_dict
        data = trans_norm(data, 'usgsFlow', stat_dict, to_norm=True)
        # cut the first rho data to match generated flow time series
        rho = self.data_config.model_dict["train"]["miniBatch"][1]
        data_chosen = data[:, rho - 1:, :]
        nx = data_chosen.shape[0]
        ny_per_nx = data_chosen.shape[1] - rho + 1
        x_tensor = torch.zeros([nx * ny_per_nx, rho, 1], requires_grad=False)
        for i in range(nx):
            per_x_np = np.zeros([ny_per_nx, rho, 1])
            for j in range(ny_per_nx):
                per_x_np[j, :, :] = data_chosen[i, j:j + rho, :]
            x_tensor[(i * ny_per_nx):((i + 1) * ny_per_nx), :, :] = torch.from_numpy(per_x_np)
        print("outflow ready!")
        return x_tensor

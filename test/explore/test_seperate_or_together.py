import unittest

from functools import reduce

import matplotlib

import definitions
from data import GagesConfig, GagesSource, DataModel
from data.data_input import save_datamodel, GagesModel, _basin_norm, save_result, load_result
from data.gages_input_dataset import load_dataconfig_case_exp, load_pub_test_result, load_ensemble_result
from data.nid_input import NidModel, save_nidinput
from explore.gages_stat import split_results_to_regions
from explore.stat import statError, ecdf
import numpy as np
import os
import pandas as pd

from utils import serialize_json, unserialize_json
from utils.dataset_format import subset_of_dict
from utils.hydro_math import is_any_elem_in_a_lst
from visual import plot_ts_obs_pred
from visual.plot_model import plot_ind_map, plot_we_need, plot_map, plot_gages_map_and_ts
from visual.plot_stat import plot_ecdf, plot_diff_boxes, plot_ecdfs, swarmplot_without_legend
import seaborn as sns
import matplotlib.pyplot as plt


class MyTestCase(unittest.TestCase):
    """data pre-process and post-process"""

    def setUp(self) -> None:
        """before all of these, natural flow model need to be generated by config.ini of gages dataset, and it need
        to be moved to right dir manually """
        config_dir = definitions.CONFIG_DIR
        # self.config_file = os.path.join(config_dir, "basic/config_exp23.ini")
        # self.subdir = r"basic/exp23"
        self.config_file = os.path.join(config_dir, "basic/config_exp12.ini")
        self.subdir = r"basic/exp12"
        self.random_seed = 1234
        self.config_data = GagesConfig.set_subdir(self.config_file, self.subdir)
        # self.nid_file = 'PA_U.xlsx'
        # self.nid_file = 'OH_U.xlsx'
        self.nid_file = 'NID2018_U.xlsx'
        self.test_epoch = 300
        self.split_num = 1  # just for testing 1 case

    def test_comp_pub_results(self):
        config_dir = definitions.CONFIG_DIR
        config_file_a = os.path.join(config_dir, "ecoregion/config_exp4.ini")
        subdir_a = r"ecoregion/exp4"
        config_data_a = GagesConfig.set_subdir(config_file_a, subdir_a)
        config_file_b = os.path.join(config_dir, "ecoregion/config_exp5.ini")
        subdir_b = r"ecoregion/exp5"
        config_data_b = GagesConfig.set_subdir(config_file_b, subdir_b)
        train_set = "train_dataset"
        test_set = "test_dataset"
        show_ind_key = "NSE"
        for i in range(self.split_num):
            inds_df_a, inds_majordam_df_a = load_pub_test_result(config_data_a, i, self.test_epoch)
            inds_df_b, inds_majordam_df_b = load_pub_test_result(config_data_b, i, self.test_epoch)
            df_a = pd.DataFrame({train_set: np.full([inds_df_a.shape[0]], 'dataset A'),
                                 test_set: np.full([inds_df_a.shape[0]], 'dataset B'),
                                 show_ind_key: inds_df_a[show_ind_key]})
            df_a_largedor = pd.DataFrame({train_set: np.full([inds_majordam_df_a.shape[0]], 'dataset A'),
                                          test_set: np.full([inds_majordam_df_a.shape[0]], 'dataset C'),
                                          show_ind_key: inds_majordam_df_a[show_ind_key]})
            df_b = pd.DataFrame({train_set: np.full([inds_df_b.shape[0]], 'dataset A+'),
                                 test_set: np.full([inds_df_b.shape[0]], 'dataset B'),
                                 show_ind_key: inds_df_b[show_ind_key]})
            df_b_largedor = pd.DataFrame({train_set: np.full([inds_majordam_df_b.shape[0]], 'dataset A+'),
                                          test_set: np.full([inds_majordam_df_b.shape[0]], 'dataset C'),
                                          show_ind_key: inds_majordam_df_b[show_ind_key]})
            frames = [df_a, df_a_largedor, df_b, df_b_largedor]
            result = pd.concat(frames)
            sns_box = sns.boxplot(x=train_set, y=show_ind_key, hue=test_set, data=result, showfliers=False)
            sns.despine(offset=10, trim=True)
            plt.show()

    def test_diversion_pooling_seperate_cases(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        diversion_yes = True
        diversion_no = False
        source_data_diversion = GagesSource.choose_some_basins(self.config_data,
                                                               self.config_data.model_dict["data"]["tRangeTrain"],
                                                               screen_basin_area_huc4=False,
                                                               diversion=diversion_yes)
        source_data_nodivert = GagesSource.choose_some_basins(self.config_data,
                                                              self.config_data.model_dict["data"]["tRangeTrain"],
                                                              screen_basin_area_huc4=False,
                                                              diversion=diversion_no)
        sites_id_nodivert = source_data_nodivert.all_configs['flow_screen_gage_id']
        sites_id_diversion = source_data_diversion.all_configs['flow_screen_gage_id']

        idx_lst_nodivert = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_nodivert]
        idx_lst_diversion = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_diversion]

        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds = statError(obs, pred)
        inds_df = pd.DataFrame(inds)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["no_diversion_together", "diversion_together"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_nodivert])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_diversion])
        xs.append(x2)
        ys.append(y2)

        cases_exps = ["diversion_exp2", "diversion_exp1"]
        cases_exps_legends_separate = ["no_diversion_separate", "diversion_separate"]
        for case_exp in cases_exps:
            config_data_i = load_dataconfig_case_exp(case_exp)
            pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            inds_i = statError(obs_i, pred_i)
            x, y = ecdf(inds_i[keys_nse])
            xs.append(x)
            ys.append(y)

        plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate)

    def test_diversion_small_dor_pooling_cases(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        diversion_no = False
        source_data_nodivert = GagesSource.choose_some_basins(self.config_data,
                                                              self.config_data.model_dict["data"]["tRangeTrain"],
                                                              screen_basin_area_huc4=False,
                                                              diversion=diversion_no)
        sites_id_nodivert = source_data_nodivert.all_configs['flow_screen_gage_id']

        dor_1 = - 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']

        no_divert_small_dor = np.intersect1d(sites_id_nodivert, sites_id_dor1)

        idx_lst_nodivert_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in no_divert_small_dor]
        idx_lst_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor1]

        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds = statError(obs, pred)
        inds_df = pd.DataFrame(inds)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["not_diverted_small_dor", "small_dor"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_nodivert_smalldor])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_smalldor])
        xs.append(x2)
        ys.append(y2)

        plot_ecdfs(xs, ys, cases_exps_legends_together)

    def test_diversion_dor_pooling_cases(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        diversion_yes = True
        diversion_no = False
        source_data_diversion = GagesSource.choose_some_basins(self.config_data,
                                                               self.config_data.model_dict["data"]["tRangeTrain"],
                                                               screen_basin_area_huc4=False,
                                                               diversion=diversion_yes)
        source_data_nodivert = GagesSource.choose_some_basins(self.config_data,
                                                              self.config_data.model_dict["data"]["tRangeTrain"],
                                                              screen_basin_area_huc4=False,
                                                              diversion=diversion_no)
        sites_id_nodivert = source_data_nodivert.all_configs['flow_screen_gage_id']
        sites_id_diversion = source_data_diversion.all_configs['flow_screen_gage_id']

        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']

        no_divert_small_dor = np.intersect1d(sites_id_nodivert, sites_id_dor1)
        no_divert_large_dor = np.intersect1d(sites_id_nodivert, sites_id_dor2)
        diversion_small_dor = np.intersect1d(sites_id_diversion, sites_id_dor1)
        diversion_large_dor = np.intersect1d(sites_id_diversion, sites_id_dor2)

        idx_lst_nodivert_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in no_divert_small_dor]
        idx_lst_nodivert_largedor = [i for i in range(len(all_sites)) if all_sites[i] in no_divert_large_dor]
        idx_lst_diversion_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in diversion_small_dor]
        idx_lst_diversion_largedor = [i for i in range(len(all_sites)) if all_sites[i] in diversion_large_dor]

        # pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        # pred = pred.reshape(pred.shape[0], pred.shape[1])
        # obs = obs.reshape(pred.shape[0], pred.shape[1])
        # inds = statError(obs, pred)
        # inds_df = pd.DataFrame(inds)
        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["not_diverted_small_dor", "not_diverted_large_dor", "diversion_small_dor",
                                       "diversion_large_dor"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_nodivert_smalldor])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_nodivert_largedor])
        xs.append(x2)
        ys.append(y2)

        x3, y3 = ecdf(inds_df[keys_nse].iloc[idx_lst_diversion_smalldor])
        xs.append(x3)
        ys.append(y3)

        x4, y4 = ecdf(inds_df[keys_nse].iloc[idx_lst_diversion_largedor])
        xs.append(x4)
        ys.append(y4)

        plot_ecdfs(xs, ys, cases_exps_legends_together)

    def test_plot_3_factors_catplot(self):
        config_dir = definitions.CONFIG_DIR
        config_file = os.path.join(config_dir, "basic/config_exp12.ini")
        subdir = r"basic/exp12"
        random_seed = 1234
        test_epoch = 300
        # test_epoch = 20
        config_data = GagesConfig.set_subdir(config_file, subdir)
        data_model = GagesModel.load_datamodel(config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        attr_lst = ["RUNAVE7100", "STOR_NOR_2009"]
        usgs_id = data_model.t_s_dict["sites_id"]
        attrs_runavg_stor = data_model.data_source.read_attr(usgs_id, attr_lst, is_return_dict=False)
        run_avg = attrs_runavg_stor[:, 0] * (10 ** (-3)) * (10 ** 6)  # m^3 per year
        nor_storage = attrs_runavg_stor[:, 1] * 1000  # m^3
        dors_value = nor_storage / run_avg
        dors = np.full(len(usgs_id), "dor<0.02")
        for i in range(len(usgs_id)):
            if dors_value[i] >= 0.02:
                dors[i] = "dor≥0.02"

        diversions = np.full(len(usgs_id), "no ")
        diversion_strs = ["diversion", "divert"]
        attr_lst = ["WR_REPORT_REMARKS", "SCREENING_COMMENTS"]
        data_attr = data_model.data_source.read_attr_origin(usgs_id, attr_lst)
        diversion_strs_lower = [elem.lower() for elem in diversion_strs]
        data_attr0_lower = np.array([elem.lower() if type(elem) == str else elem for elem in data_attr[0]])
        data_attr1_lower = np.array([elem.lower() if type(elem) == str else elem for elem in data_attr[1]])
        data_attr_lower = np.vstack((data_attr0_lower, data_attr1_lower)).T
        for i in range(len(usgs_id)):
            if is_any_elem_in_a_lst(diversion_strs_lower, data_attr_lower[i], include=True):
                diversions[i] = "yes"

        nid_dir = os.path.join("/".join(config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        purpose_regions = {}
        for i in range(gage_main_dam_purpose_unique.size):
            sites_id = []
            for key, value in gage_main_dam_purpose.items():
                if value == gage_main_dam_purpose_unique[i]:
                    sites_id.append(key)
            assert (all(x < y for x, y in zip(sites_id, sites_id[1:])))
            purpose_regions[gage_main_dam_purpose_unique[i]] = sites_id
        id_regions_idx = []
        id_regions_sites_ids = []
        regions_name = []
        show_min_num = 10
        df_id_region = np.array(data_model.t_s_dict["sites_id"])
        for key, value in purpose_regions.items():
            gages_id = value
            c, ind1, ind2 = np.intersect1d(df_id_region, gages_id, return_indices=True)
            if c.size < show_min_num:
                continue
            assert (all(x < y for x, y in zip(ind1, ind1[1:])))
            assert (all(x < y for x, y in zip(c, c[1:])))
            id_regions_idx.append(ind1)
            id_regions_sites_ids.append(c)
            regions_name.append(key)
        preds, obss, inds_dfs = split_results_to_regions(data_model, test_epoch, id_regions_idx,
                                                         id_regions_sites_ids)
        frames = []
        x_name = "purposes"
        y_name = "NSE"
        hue_name = "DOR"
        col_name = "diversion"
        for i in range(len(id_regions_idx)):
            # plot box，使用seaborn库
            keys = ["NSE"]
            inds_test = subset_of_dict(inds_dfs[i], keys)
            inds_test = inds_test[keys[0]].values
            df_dict_i = {}
            str_i = regions_name[i]
            df_dict_i[x_name] = np.full([inds_test.size], str_i)
            df_dict_i[y_name] = inds_test
            df_dict_i[hue_name] = dors[id_regions_idx[i]]
            df_dict_i[col_name] = diversions[id_regions_idx[i]]
            # df_dict_i[hue_name] = nor_storage[id_regions_idx[i]]
            df_i = pd.DataFrame(df_dict_i)
            frames.append(df_i)
        result = pd.concat(frames)
        matplotlib.use('TkAgg')
        # g = sns.catplot(x=x_name, y=y_name, hue=hue_name, col=col_name,
        #                 data=result, kind="swarm",
        #                 height=4, aspect=.7)

        g = sns.catplot(x=x_name, y=y_name,
                        hue=hue_name, col=col_name,
                        data=result, palette="Set1",
                        kind="box", dodge=True, showfliers=False)
        # g.set(ylim=(-1, 1))

        plt.show()

    def test_plot_3_factors_colorbar(self):
        # TODO: there is something wrong when using facetgrid and searmplot at the same time
        config_dir = definitions.CONFIG_DIR
        config_file = os.path.join(config_dir, "basic/config_exp11.ini")
        subdir = r"basic/exp11"
        random_seed = 1234
        # test_epoch = 300
        test_epoch = 20
        config_data = GagesConfig.set_subdir(config_file, subdir)
        data_model = GagesModel.load_datamodel(config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        attr_lst = ["RUNAVE7100", "STOR_NOR_2009"]
        usgs_id = data_model.t_s_dict["sites_id"]
        attrs_runavg_stor = data_model.data_source.read_attr(usgs_id, attr_lst, is_return_dict=False)
        run_avg = attrs_runavg_stor[:, 0] * (10 ** (-3)) * (10 ** 6)  # m^3 per year
        nor_storage = attrs_runavg_stor[:, 1] * 1000  # m^3
        dors = nor_storage / run_avg

        diversions = np.full(len(usgs_id), "no_diversion")
        diversion_strs = ["diversion", "divert"]
        attr_lst = ["WR_REPORT_REMARKS", "SCREENING_COMMENTS"]
        data_attr = data_model.data_source.read_attr_origin(usgs_id, attr_lst)
        diversion_strs_lower = [elem.lower() for elem in diversion_strs]
        data_attr0_lower = np.array([elem.lower() if type(elem) == str else elem for elem in data_attr[0]])
        data_attr1_lower = np.array([elem.lower() if type(elem) == str else elem for elem in data_attr[1]])
        data_attr_lower = np.vstack((data_attr0_lower, data_attr1_lower)).T
        for i in range(len(usgs_id)):
            if is_any_elem_in_a_lst(diversion_strs_lower, data_attr_lower[i], include=True):
                diversions[i] = "diversion"

        nid_dir = os.path.join("/".join(config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        purpose_regions = {}
        for i in range(gage_main_dam_purpose_unique.size):
            sites_id = []
            for key, value in gage_main_dam_purpose.items():
                if value == gage_main_dam_purpose_unique[i]:
                    sites_id.append(key)
            assert (all(x < y for x, y in zip(sites_id, sites_id[1:])))
            purpose_regions[gage_main_dam_purpose_unique[i]] = sites_id
        id_regions_idx = []
        id_regions_sites_ids = []
        regions_name = []
        show_min_num = 10
        df_id_region = np.array(data_model.t_s_dict["sites_id"])
        for key, value in purpose_regions.items():
            gages_id = value
            c, ind1, ind2 = np.intersect1d(df_id_region, gages_id, return_indices=True)
            if c.size < show_min_num:
                continue
            assert (all(x < y for x, y in zip(ind1, ind1[1:])))
            assert (all(x < y for x, y in zip(c, c[1:])))
            id_regions_idx.append(ind1)
            id_regions_sites_ids.append(c)
            regions_name.append(key)
        preds, obss, inds_dfs = split_results_to_regions(data_model, test_epoch, id_regions_idx,
                                                         id_regions_sites_ids)
        frames = []
        x_name = "purposes"
        y_name = "NSE"
        hue_name = "DOR"
        col_name = "diversion"
        for i in range(len(id_regions_idx)):
            # plot box，使用seaborn库
            keys = ["NSE"]
            inds_test = subset_of_dict(inds_dfs[i], keys)
            inds_test = inds_test[keys[0]].values
            df_dict_i = {}
            str_i = regions_name[i]
            df_dict_i[x_name] = np.full([inds_test.size], str_i)
            df_dict_i[y_name] = inds_test
            df_dict_i[hue_name] = dors[id_regions_idx[i]]
            df_dict_i[col_name] = diversions[id_regions_idx[i]]
            # df_dict_i[hue_name] = nor_storage[id_regions_idx[i]]
            df_i = pd.DataFrame(df_dict_i)
            frames.append(df_i)
        result = pd.concat(frames)
        matplotlib.use('TkAgg')
        g = sns.FacetGrid(result, col=col_name)  # ,  palette = 'seismic'
        plt.title('Distribution of different purposes')
        vmin = result[hue_name].min()
        vmax = result[hue_name].max()
        cmap = sns.diverging_palette(240, 10, l=65, center="dark", as_cmap=True)

        g = g.map(swarmplot_without_legend, x_name, y_name, hue_name, vmin=vmin, vmax=vmax, cmap=cmap)

        # Make space for the colorbar
        g.fig.subplots_adjust(right=.92)

        # Define a new Axes where the colorbar will go
        cax = g.fig.add_axes([.94, .25, .02, .6])

        # Get a mappable object with the same colormap as the data
        points = plt.scatter([], [], c=[], vmin=vmin, vmax=vmax, cmap=cmap)

        # Draw the colorbar
        g.fig.colorbar(points, cax=cax)
        plt.show()

    def test_export_result(self):
        # data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
        #                                        data_source_file_name='test_data_source.txt',
        #                                        stat_file_name='test_Statistics.json', flow_file_name='test_flow.npy',
        #                                        forcing_file_name='test_forcing.npy', attr_file_name='test_attr.npy',
        #                                        f_dict_file_name='test_dictFactorize.json',
        #                                        var_dict_file_name='test_dictAttribute.json',
        #                                        t_s_dict_file_name='test_dictTimeSpace.json')

        config_dir = definitions.CONFIG_DIR
        # camels_config_file = os.path.join(config_dir, "basic/config_exp1.ini")
        # camels_subdir = r"basic/exp1"
        # camels_config_data = GagesConfig.set_subdir(camels_config_file, camels_subdir)
        # data_model = GagesModel.load_datamodel(camels_config_data.data_path["Temp"],
        #                                        data_source_file_name='test_data_source.txt',
        #                                        stat_file_name='test_Statistics.json',
        #                                        flow_file_name='test_flow.npy',
        #                                        forcing_file_name='test_forcing.npy',
        #                                        attr_file_name='test_attr.npy',
        #                                        f_dict_file_name='test_dictFactorize.json',
        #                                        var_dict_file_name='test_dictAttribute.json',
        #                                        t_s_dict_file_name='test_dictTimeSpace.json')

        # config_file = os.path.join(config_dir, "nodam/config_exp3.ini")
        # subdir = r"nodam/exp3"
        config_file = os.path.join(config_dir, "majordam/config_exp2.ini")
        subdir = r"majordam/exp2"
        config_data = GagesConfig.set_subdir(config_file, subdir)
        data_model = GagesModel.load_datamodel(config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(obs.shape[0], obs.shape[1])
        inds = statError(obs, pred)
        inds['STAID'] = data_model.t_s_dict["sites_id"]
        inds_df = pd.DataFrame(inds)
        print(inds_df.median())

        # inds_df.to_csv(os.path.join(self.config_data.data_path["Out"], 'data_df.csv'))

    def test_conus_ensemble_result_for_smalldor_and_largedor_separate_and_together(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']
        idx_lst_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor1]
        idx_lst_largedor = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor2]

        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)
        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["small_dor", "large_dor"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_smalldor])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_largedor])
        xs.append(x2)
        ys.append(y2)

        # compare_item = 0
        compare_item = 1
        if compare_item == 0:
            plot_ecdfs(xs, ys, cases_exps_legends_together)
        elif compare_item == 1:
            cases_exps = ["dam_exp1", "dam_exp2", "dam_exp3", "dam_exp7", "dam_exp8", "dam_exp9"]
            inds_df_smalldor = load_ensemble_result(cases_exps, self.test_epoch)
            x3, y3 = ecdf(inds_df_smalldor[keys_nse])
            xs.append(x3)
            ys.append(y3)
            cases_exps = ["dam_exp4", "dam_exp5", "dam_exp6", "dam_exp13", "dam_exp16", "dam_exp19"]
            inds_df_largedor = load_ensemble_result(cases_exps, self.test_epoch)
            x4, y4 = ecdf(inds_df_largedor[keys_nse])
            xs.append(x4)
            ys.append(y4)
            cases_exps_legends_separate = ["small_dor", "large_dor"]
            plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate,
                       style=["together", "together", "separate", "separate"], case_str="dor_value",
                       event_str="is_pooling_together", x_str="NSE", y_str="CDF")

    def test_conus_ensemble_result_for_smalldor(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']
        idx_lst_smalldor = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor1]
        idx_lst_largedor = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor2]

        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends = ["all_basins", "basins_with_small_dor", "basins_with_large_dor"]

        x1, y1 = ecdf(inds_df[keys_nse])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_smalldor])
        xs.append(x2)
        ys.append(y2)

        x3, y3 = ecdf(inds_df[keys_nse].iloc[idx_lst_largedor])
        xs.append(x3)
        ys.append(y3)

        plot_ecdfs(xs, ys, cases_exps_legends, x_str="NSE", y_str="CDF")

    def test_conus_result_for_camels531(self):
        camels531_gageid_file = os.path.join(self.config_data.data_path["DB"], "camels531", "CAMELS531.txt")
        gauge_df = pd.read_csv(camels531_gageid_file, dtype={"GaugeID": str})
        gauge_list = gauge_df["GaugeID"].values
        all_sites_camels_531 = np.sort([str(gauge).zfill(8) for gauge in gauge_list])

        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]

        idx_lst_camels = [i for i in range(len(all_sites)) if all_sites[i] in all_sites_camels_531]
        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends = ["521sites_in_CAMELS_trained_in_CONUS", "trained_only_in_521sites_in_CAMELS"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_camels])
        xs.append(x1)
        ys.append(y1)

        cases_exps_camels = ["basic_exp31", "basic_exp32", "basic_exp33", "basic_exp34", "basic_exp35", "basic_exp36"]
        inds_df_camels = load_ensemble_result(cases_exps_camels, self.test_epoch)
        x2, y2 = ecdf(inds_df_camels[keys_nse])
        xs.append(x2)
        ys.append(y2)

        plot_ecdfs(xs, ys, cases_exps_legends, x_str="NSE", y_str="CDF")

    def test_conus_result_for_camels_dataset(self):
        config_dir = definitions.CONFIG_DIR
        camels_config_file = os.path.join(config_dir, "basic/config_exp1.ini")
        camels_subdir = r"basic/exp1"
        camels_config_data = GagesConfig.set_subdir(camels_config_file, camels_subdir)
        camels_data_model = GagesModel.load_datamodel(camels_config_data.data_path["Temp"],
                                                      data_source_file_name='test_data_source.txt',
                                                      stat_file_name='test_Statistics.json',
                                                      flow_file_name='test_flow.npy',
                                                      forcing_file_name='test_forcing.npy',
                                                      attr_file_name='test_attr.npy',
                                                      f_dict_file_name='test_dictFactorize.json',
                                                      var_dict_file_name='test_dictAttribute.json',
                                                      t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites_camels = camels_data_model.t_s_dict["sites_id"]

        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]
        idx_lst_camels = [i for i in range(len(all_sites)) if all_sites[i] in all_sites_camels]

        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends = ["sites_in_CAMELS_trained_in_CONUS", "trained_only_in_CAMELS"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_camels])
        xs.append(x1)
        ys.append(y1)

        cases_exps_camels = ["basic_exp1", "basic_exp26", "basic_exp27", "basic_exp28", "basic_exp29", "basic_exp30"]
        inds_df_camels = load_ensemble_result(cases_exps_camels, self.test_epoch)
        x2, y2 = ecdf(inds_df_camels["NSE"])
        xs.append(x2)
        ys.append(y2)

        plot_ecdfs(xs, ys, cases_exps_legends, x_str="NSE", y_str="CDF")

    def test_ensemble_conus_ecdf(self):
        cases_exps = ["basic_exp12", "basic_exp13", "basic_exp14", "basic_exp15", "basic_exp16", "basic_exp18"]
        inds_df = load_ensemble_result(cases_exps, self.test_epoch)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends = ["sites_in_CONUS"]

        x1, y1 = ecdf(inds_df[keys_nse])
        xs.append(x1)
        ys.append(y1)

        plot_ecdfs(xs, ys, cases_exps_legends)

    def test_ensemble_camels_mapts(self):
        config_dir = definitions.CONFIG_DIR
        camels_config_file = os.path.join(config_dir, "basic/config_exp1.ini")
        camels_subdir = r"basic/exp1"
        camels_config_data = GagesConfig.set_subdir(camels_config_file, camels_subdir)
        data_model = GagesModel.load_datamodel(camels_config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')

        # plot map ts
        show_ind_key = 'NSE'
        cases_exps_camels = ["basic_exp1", "basic_exp26", "basic_exp27", "basic_exp28", "basic_exp29", "basic_exp30"]
        inds_df, pred_mean, obs_mean = load_ensemble_result(cases_exps_camels, self.test_epoch, return_value=True)

        # nse_range = [0.5, 1]
        nse_range = [0, 1]
        # nse_range = [-10000, 1]
        # nse_range = [-10000, 0]
        idx_lst_nse = inds_df[
            (inds_df[show_ind_key] >= nse_range[0]) & (inds_df[show_ind_key] < nse_range[1])].index.tolist()
        plot_gages_map_and_ts(data_model, obs_mean, pred_mean, inds_df, show_ind_key, idx_lst_nse,
                              pertile_range=[0, 100])

    def test_camels_dataset(self):
        config_dir = definitions.CONFIG_DIR
        camels_config_file = os.path.join(config_dir, "basic/config_exp1.ini")
        camels_subdir = r"basic/exp1"
        camels_config_data = GagesConfig.set_subdir(camels_config_file, camels_subdir)
        camels_data_model = GagesModel.load_datamodel(camels_config_data.data_path["Temp"],
                                                      data_source_file_name='test_data_source.txt',
                                                      stat_file_name='test_Statistics.json',
                                                      flow_file_name='test_flow.npy',
                                                      forcing_file_name='test_forcing.npy',
                                                      attr_file_name='test_attr.npy',
                                                      f_dict_file_name='test_dictFactorize.json',
                                                      var_dict_file_name='test_dictAttribute.json',
                                                      t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites_camels = camels_data_model.t_s_dict["sites_id"]
        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']
        print(np.intersect1d(all_sites_camels, sites_id_dor1))
        print(np.intersect1d(all_sites_camels, sites_id_dor2))

        attr_lst = ["RUNAVE7100", "STOR_NOR_2009"]
        data_attr, var_dict, f_dict = camels_data_model.data_source.read_attr(
            np.intersect1d(all_sites_camels, sites_id_dor2), attr_lst)
        run_avg = data_attr[:, 0] * (10 ** (-3)) * (10 ** 6)  # m^3 per year
        nor_storage = data_attr[:, 1] * 1000  # m^3
        dors = nor_storage / run_avg
        print(dors)

    def test_plot_ecdf_together(self):
        xs = []
        ys = []
        # cases_exps = ["basic_exp21", "basic_exp22", "basic_exp23", "basic_exp24"]
        # cases_exps_legends = ["miniBatch = [100, 200]	hiddenSize = 256",
        #                       "miniBatch = [100, 100]	hiddenSize = 256",
        #                       "miniBatch = [100, 365]	hiddenSize = 256",
        #                       "miniBatch = [100, 365]	hiddenSize = 128"]

        # cases_exps = ["basic_exp19", "basic_exp20", "basic_exp23", "basic_exp25"]
        # cases_exps_legends = ["attr19", "attr20", "attr23", "attr25"]

        cases_exps = ["basic_exp25", "basic_exp20", "basic_exp19", "basic_exp12"]
        cases_exps_legends = ["attr_comb_1", "attr_comb_2", "attr_comb_3", "attr_comb_4"]

        # cases_exps = ["basic_exp1", "basic_exp2", "basic_exp3", "basic_exp4"]
        # cases_exps_legends = ["attr1", "attr2", "attr3", "attr4"]

        # cases_exps = ["basic_exp1", "basic_exp5", "basic_exp6", "basic_exp7", "basic_exp8", "basic_exp9", "basic_exp10"]
        # cases_exps_legends = ["basic_exp1", "basic_exp5", "basic_exp6", "basic_exp7", "basic_exp8", "basic_exp9",
        #                       "basic_exp10"]
        for case_exp in cases_exps:
            config_data_i = load_dataconfig_case_exp(case_exp)
            pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            inds_i = statError(obs_i, pred_i)
            x, y = ecdf(inds_i["NSE"])
            xs.append(x)
            ys.append(y)
        plot_ecdfs(xs, ys, cases_exps_legends)

    def test_plot_boxes_together(self):
        cases_exps = ["basic_exp25", "basic_exp20", "basic_exp19", "basic_exp12"]
        cases_exps_legends = ["attr_comb_1", "attr_comb_2", "attr_comb_3", "attr_comb_4"]
        show_ind_key = "NSE"
        attr = "attributes_combination"
        frames = []
        for i in range(len(cases_exps)):
            config_data_i = load_dataconfig_case_exp(cases_exps[i])
            pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            inds_i = pd.DataFrame(statError(obs_i, pred_i))
            df_i = pd.DataFrame({attr: np.full([inds_i.shape[0]], cases_exps_legends[i]),
                                 show_ind_key: inds_i[show_ind_key]})
            frames.append(df_i)
        result = pd.concat(frames)
        sns_box = sns.boxplot(x=attr, y=show_ind_key, data=result, showfliers=False)
        sns.despine(offset=10, trim=True)
        plt.show()

    def test_multi_cases_in_one_exp_stat_together(self):
        experiment = "ecoregion_exp1"
        cases = ["5.2", "5.3", "6.2", "7.1", "8.1", "8.2", "8.3", "8.4", "8.5", "9.2", "9.3", "9.4", "9.5",
                 "9.6", "10.1", "10.2", "10.4", "11.1", "12.1", "13.1"]
        pred = []
        obs = []
        config_data_i = load_dataconfig_case_exp(experiment)
        for case in cases:
            temp_dir = os.path.join(config_data_i.data_path['Temp'], case)
            pred_i, obs_i = load_result(temp_dir, self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            pred.append(pred_i)
            obs.append(obs_i)
        pred_stack = reduce(lambda a, b: np.vstack((a, b)),
                            list(map(lambda x: x.reshape(x.shape[0], x.shape[1]), pred)))
        obs_stack = reduce(lambda a, b: np.vstack((a, b)),
                           list(map(lambda x: x.reshape(x.shape[0], x.shape[1]), obs)))
        inds = statError(obs_stack, pred_stack)
        inds_df = pd.DataFrame(inds)
        print(inds_df.median(axis=0))
        print(inds_df.mean(axis=0))

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends = ["separate", "together"]

        x1, y1 = ecdf(inds_df[keys_nse])
        xs.append(x1)
        ys.append(y1)

        pooling_experiment = "basic_exp23"
        config_data_pooling = load_dataconfig_case_exp(pooling_experiment)
        pred, obs = load_result(config_data_pooling.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds_pooling = statError(obs, pred)
        inds_pooling_df = pd.DataFrame(inds_pooling)
        x2, y2 = ecdf(inds_pooling_df[keys_nse])
        xs.append(x2)
        ys.append(y2)
        plot_ecdfs(xs, ys, cases_exps_legends)

    def test_multi_cases_stat_together(self):
        cases_exps = ["basic_exp14", "basic_exp15", "basic_exp2", "basic_exp3", "basic_exp4",
                      "basic_exp5", "basic_exp6", "basic_exp7", "basic_exp8", "basic_exp9"]
        pred = []
        obs = []
        for case_exp in cases_exps:
            config_data_i = load_dataconfig_case_exp(case_exp)
            pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            pred.append(pred_i)
            obs.append(obs_i)
        pred_stack = reduce(lambda a, b: np.vstack((a, b)),
                            list(map(lambda x: x.reshape(x.shape[0], x.shape[1]), pred)))
        obs_stack = reduce(lambda a, b: np.vstack((a, b)),
                           list(map(lambda x: x.reshape(x.shape[0], x.shape[1]), obs)))
        inds = statError(obs_stack, pred_stack)
        inds_df = pd.DataFrame(inds)
        print(inds_df.median(axis=0))
        print(inds_df.mean(axis=0))

    def test_purposes_seperate(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "conus-all_90-10_nan-0.0_00-1.0")
        data_model_test = GagesModel.load_datamodel(data_dir,
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')
        data_model = GagesModel.update_data_model(self.config_data, data_model_test)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        purpose_regions = {}
        for i in range(gage_main_dam_purpose_unique.size):
            sites_id = []
            for key, value in gage_main_dam_purpose.items():
                if value == gage_main_dam_purpose_unique[i]:
                    sites_id.append(key)
            assert (all(x < y for x, y in zip(sites_id, sites_id[1:])))
            purpose_regions[gage_main_dam_purpose_unique[i]] = sites_id
        id_regions_idx = []
        id_regions_sites_ids = []
        df_id_region = np.array(data_model.t_s_dict["sites_id"])
        for key, value in purpose_regions.items():
            gages_id = value
            c, ind1, ind2 = np.intersect1d(df_id_region, gages_id, return_indices=True)
            assert (all(x < y for x, y in zip(ind1, ind1[1:])))
            assert (all(x < y for x, y in zip(c, c[1:])))
            id_regions_idx.append(ind1)
            id_regions_sites_ids.append(c)
        pred_all, obs_all = load_result(self.config_data.data_path["Temp"], self.test_epoch)
        pred_all = pred_all.reshape(pred_all.shape[0], pred_all.shape[1])
        obs_all = obs_all.reshape(obs_all.shape[0], obs_all.shape[1])
        for i in range(9, len(gage_main_dam_purpose_unique)):
            pred = pred_all[id_regions_idx[i], :]
            obs = obs_all[id_regions_idx[i], :]
            inds = statError(obs, pred)
            inds['STAID'] = id_regions_sites_ids[i]
            inds_df = pd.DataFrame(inds)
            inds_df.to_csv(os.path.join(self.config_data.data_path["Out"],
                                        gage_main_dam_purpose_unique[i] + "epoch" + str(
                                            self.test_epoch) + 'data_df.csv'))
            # plot box，使用seaborn库
            keys = ["Bias", "RMSE", "NSE"]
            inds_test = subset_of_dict(inds, keys)
            box_fig = plot_diff_boxes(inds_test)
            box_fig.savefig(os.path.join(self.config_data.data_path["Out"],
                                         gage_main_dam_purpose_unique[i] + "epoch" + str(
                                             self.test_epoch) + "box_fig.png"))
            # plot ts
            sites = np.array(df_id_region[id_regions_idx[i]])
            t_range = np.array(data_model.t_s_dict["t_final_range"])
            show_me_num = 1
            ts_fig = plot_ts_obs_pred(obs, pred, sites, t_range, show_me_num)
            ts_fig.savefig(os.path.join(self.config_data.data_path["Out"],
                                        gage_main_dam_purpose_unique[i] + "epoch" + str(
                                            self.test_epoch) + "ts_fig.png"))
            # plot nse ecdf
            sites_df_nse = pd.DataFrame({"sites": sites, keys[2]: inds_test[keys[2]]})
            plot_ecdf(sites_df_nse, keys[2], os.path.join(self.config_data.data_path["Out"],
                                                          gage_main_dam_purpose_unique[i] + "epoch" + str(
                                                              self.test_epoch) + "ecdf_fig.png"))
            # plot map
            gauge_dict = data_model.data_source.gage_dict
            save_map_file = os.path.join(self.config_data.data_path["Out"],
                                         gage_main_dam_purpose_unique[i] + "epoch" + str(
                                             self.test_epoch) + "map_fig.png")
            plot_map(gauge_dict, sites_df_nse, save_file=save_map_file, id_col="STAID", lon_col="LNG_GAGE",
                     lat_col="LAT_GAGE")

    def test_purposes_inds(self):
        quick_data_dir = os.path.join(self.config_data.data_path["DB"], "quickdata")
        data_dir = os.path.join(quick_data_dir, "allnonref-dam_95-05_nan-0.1_00-1.0")
        data_model = GagesModel.load_datamodel(data_dir,
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        gages_data_model = GagesModel.update_data_model(self.config_data, data_model)
        nid_dir = os.path.join("/".join(self.config_data.data_path["DB"].split("/")[:-1]), "nid", "quickdata")
        gage_main_dam_purpose = unserialize_json(os.path.join(nid_dir, "dam_main_purpose_dict.json"))
        gage_main_dam_purpose_lst = list(gage_main_dam_purpose.values())
        gage_main_dam_purpose_unique = np.unique(gage_main_dam_purpose_lst)
        purpose_regions = {}
        for i in range(gage_main_dam_purpose_unique.size):
            sites_id = []
            for key, value in gage_main_dam_purpose.items():
                if value == gage_main_dam_purpose_unique[i]:
                    sites_id.append(key)
            assert (all(x < y for x, y in zip(sites_id, sites_id[1:])))
            purpose_regions[gage_main_dam_purpose_unique[i]] = sites_id
        id_regions_idx = []
        id_regions_sites_ids = []
        df_id_region = np.array(gages_data_model.t_s_dict["sites_id"])
        for key, value in purpose_regions.items():
            gages_id = value
            c, ind1, ind2 = np.intersect1d(df_id_region, gages_id, return_indices=True)
            assert (all(x < y for x, y in zip(ind1, ind1[1:])))
            assert (all(x < y for x, y in zip(c, c[1:])))
            id_regions_idx.append(ind1)
            id_regions_sites_ids.append(c)
        preds, obss, inds_dfs = split_results_to_regions(gages_data_model, self.test_epoch, id_regions_idx,
                                                         id_regions_sites_ids)
        region_names = list(purpose_regions.keys())
        inds_medians = []
        inds_means = []
        for i in range(len(region_names)):
            inds_medians.append(inds_dfs[i].median(axis=0))
            inds_means.append(inds_dfs[i].mean(axis=0))
        print(inds_medians)
        print(inds_means)

    def test_comp_same_dor_sim_or_basic_cases(self):
        dam_experiment1 = "dam_exp11"
        dam_experiment2 = "dam_exp12"
        config_data_dam1 = load_dataconfig_case_exp(dam_experiment1)
        data_model_dam1 = GagesModel.load_datamodel(config_data_dam1.data_path["Temp"],
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')
        config_data_dam2 = load_dataconfig_case_exp(dam_experiment2)
        data_model_dam2 = GagesModel.load_datamodel(config_data_dam2.data_path["Temp"],
                                                    data_source_file_name='test_data_source.txt',
                                                    stat_file_name='test_Statistics.json',
                                                    flow_file_name='test_flow.npy',
                                                    forcing_file_name='test_forcing.npy',
                                                    attr_file_name='test_attr.npy',
                                                    f_dict_file_name='test_dictFactorize.json',
                                                    var_dict_file_name='test_dictAttribute.json',
                                                    t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites_dam1 = data_model_dam1.t_s_dict["sites_id"]
        all_sites_dam2 = data_model_dam2.t_s_dict["sites_id"]
        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']

        source_data_nomajordam = GagesSource.choose_some_basins(self.config_data,
                                                                self.config_data.model_dict["data"]["tRangeTrain"],
                                                                screen_basin_area_huc4=False,
                                                                major_dam_num=0)
        sites_id_nomajordam = source_data_nomajordam.all_configs['flow_screen_gage_id']
        idx_lst_dor1 = [i for i in range(len(all_sites_dam1)) if
                        all_sites_dam1[i] in sites_id_dor1 and all_sites_dam1[i] not in sites_id_nomajordam]
        idx_lst_dor2 = [i for i in range(len(all_sites_dam2)) if
                        all_sites_dam2[i] in sites_id_dor2 and all_sites_dam2[i] not in sites_id_nomajordam]

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together1 = ["small_dor_major_dam", "small_dor_major_dam_natflow"]
        cases_exps_legends_together2 = ["large_dor_major_dam", "large_dor_major_dam_natflow"]

        pred_dam1, obs_dam1 = load_result(config_data_dam1.data_path['Temp'], self.test_epoch)
        pred_dam1 = pred_dam1.reshape(pred_dam1.shape[0], pred_dam1.shape[1])
        obs_dam1 = obs_dam1.reshape(pred_dam1.shape[0], pred_dam1.shape[1])
        inds_dam1 = statError(obs_dam1, pred_dam1)
        inds_dam1_df = pd.DataFrame(inds_dam1)
        x1, y1 = ecdf(inds_dam1_df[keys_nse].iloc[idx_lst_dor1])
        xs.append(x1)
        ys.append(y1)

        natflow_experiment1 = "simulate_exp3"
        config_data_natflow1 = load_dataconfig_case_exp(natflow_experiment1)
        pred1, obs1 = load_result(config_data_natflow1.data_path['Temp'], self.test_epoch)
        pred1 = pred1.reshape(pred1.shape[0], pred1.shape[1])
        obs1 = obs1.reshape(pred1.shape[0], pred1.shape[1])
        inds1 = statError(obs1, pred1)
        inds1_df = pd.DataFrame(inds1)
        x11, y11 = ecdf(inds1_df[keys_nse])
        xs.append(x11)
        ys.append(y11)

        pred_dam2, obs_dam2 = load_result(config_data_dam2.data_path['Temp'], self.test_epoch)
        pred_dam2 = pred_dam2.reshape(pred_dam2.shape[0], pred_dam2.shape[1])
        obs_dam2 = obs_dam2.reshape(pred_dam2.shape[0], pred_dam2.shape[1])
        inds_dam2 = statError(obs_dam2, pred_dam2)
        inds_dam2_df = pd.DataFrame(inds_dam2)
        x2, y2 = ecdf(inds_dam2_df[keys_nse].iloc[idx_lst_dor2])
        xs.append(x2)
        ys.append(y2)

        natflow_experiment2 = "simulate_exp4"
        config_data_natflow2 = load_dataconfig_case_exp(natflow_experiment2)
        pred2, obs2 = load_result(config_data_natflow2.data_path['Temp'], self.test_epoch)
        pred2 = pred2.reshape(pred2.shape[0], pred2.shape[1])
        obs2 = obs2.reshape(pred2.shape[0], pred2.shape[1])
        inds2 = statError(obs2, pred2)
        inds2_df = pd.DataFrame(inds2)
        x22, y22 = ecdf(inds2_df[keys_nse])
        xs.append(x22)
        ys.append(y22)

        plot_ecdfs(xs, ys, cases_exps_legends_together1 + cases_exps_legends_together2)

    def test_w_wo_majordam_pooling_seperate_cases(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]
        majordam_num1 = 0
        majordam_num2 = [1, 200]
        source_data_nomajordam = GagesSource.choose_some_basins(self.config_data,
                                                                self.config_data.model_dict["data"]["tRangeTrain"],
                                                                screen_basin_area_huc4=False,
                                                                major_dam_num=majordam_num1)
        source_data_withmajordam = GagesSource.choose_some_basins(self.config_data,
                                                                  self.config_data.model_dict["data"]["tRangeTrain"],
                                                                  screen_basin_area_huc4=False,
                                                                  major_dam_num=majordam_num2)
        sites_id_nomajordam = source_data_nomajordam.all_configs['flow_screen_gage_id']
        sites_id_withmajordam = source_data_withmajordam.all_configs['flow_screen_gage_id']
        idx_lst_nomajordam = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_nomajordam]
        idx_lst_withmajordam = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_withmajordam]

        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds = statError(obs, pred)
        inds_df = pd.DataFrame(inds)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["no_major_dam", "major_dam"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_nomajordam])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_withmajordam])
        xs.append(x2)
        ys.append(y2)

        # 0: compare w/wo in conus ; 1: 0 + major dam with natflow; 2: 0 + seperate cases
        # compare_item = 0
        compare_item = 1
        # compare_item = 2
        if compare_item == 0:
            plot_ecdfs(xs, ys, cases_exps_legends_together)
        elif compare_item == 1:
            cases_exps = ["simulate_exp2", "storage_exp1", "storage_exp2", "storage_exp3"]
            cases_exps_legends_separate = ["major_dam_natflow", "major_dam_natflow_julian", "major_dam_natflow_storage",
                                           "major_dam_natflow_storage_seq2one"]
            for case_exp in cases_exps:
                config_data_i = load_dataconfig_case_exp(case_exp)
                pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
                pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
                obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
                inds_i = statError(obs_i, pred_i)
                x, y = ecdf(inds_i[keys_nse])
                xs.append(x)
                ys.append(y)
            plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate)

        elif compare_item == 2:
            cases_exps = ["nodam_exp3", "majordam_exp2"]
            cases_exps_legends_separate = ["no_major_dam", "major_dam"]
            for case_exp in cases_exps:
                config_data_i = load_dataconfig_case_exp(case_exp)
                pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
                pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
                obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
                inds_i = statError(obs_i, pred_i)
                x, y = ecdf(inds_i[keys_nse])
                xs.append(x)
                ys.append(y)

            plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate,
                       style=["together", "together", "separate", "separate"])

    def test_dor_seperate(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]
        dor_1 = - 0.02
        dor_2 = 0.02
        source_data_dor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_1)
        source_data_dor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          screen_basin_area_huc4=False,
                                                          DOR=dor_2)
        sites_id_dor1 = source_data_dor1.all_configs['flow_screen_gage_id']
        sites_id_dor2 = source_data_dor2.all_configs['flow_screen_gage_id']
        idx_lst_dor1 = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor1]
        idx_lst_dor2 = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_dor2]

        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds = statError(obs, pred)
        inds_df = pd.DataFrame(inds)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["small_dor", "large_dor"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_dor1])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_dor2])
        xs.append(x2)
        ys.append(y2)

        # compare_item = 0
        compare_item = 1
        if compare_item == 0:
            plot_ecdfs(xs, ys, cases_exps_legends_together)
        elif compare_item == 1:
            # cases_exps = ["dam_exp17", "dam_exp18"]
            cases_exps = ["dam_exp1", "dam_exp4"]
            cases_exps_legends_separate = ["small_dor", "large_dor"]
            for case_exp in cases_exps:
                config_data_i = load_dataconfig_case_exp(case_exp)
                pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
                pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
                obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
                inds_i = statError(obs_i, pred_i)
                x, y = ecdf(inds_i[keys_nse])
                xs.append(x)
                ys.append(y)

            plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate,
                       style=["together", "together", "separate", "separate"], case_str="dor_value",
                       event_str="is_pooling_together", x_str="NSE", y_str="CDF")

    def test_stor_seperate(self):
        data_model = GagesModel.load_datamodel(self.config_data.data_path["Temp"],
                                               data_source_file_name='test_data_source.txt',
                                               stat_file_name='test_Statistics.json',
                                               flow_file_name='test_flow.npy',
                                               forcing_file_name='test_forcing.npy',
                                               attr_file_name='test_attr.npy',
                                               f_dict_file_name='test_dictFactorize.json',
                                               var_dict_file_name='test_dictAttribute.json',
                                               t_s_dict_file_name='test_dictTimeSpace.json')
        all_sites = data_model.t_s_dict["sites_id"]
        storage_nor_1 = [0, 50]
        storage_nor_2 = [50, 15000]  # max is 14348.6581036888
        source_data_nor1 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          STORAGE=storage_nor_1)
        source_data_nor2 = GagesSource.choose_some_basins(self.config_data,
                                                          self.config_data.model_dict["data"]["tRangeTrain"],
                                                          STORAGE=storage_nor_2)
        sites_id_nor1 = source_data_nor1.all_configs['flow_screen_gage_id']
        sites_id_nor2 = source_data_nor2.all_configs['flow_screen_gage_id']
        idx_lst_nor1 = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_nor1]
        idx_lst_nor2 = [i for i in range(len(all_sites)) if all_sites[i] in sites_id_nor2]

        pred, obs = load_result(data_model.data_source.data_config.data_path['Temp'], self.test_epoch)
        pred = pred.reshape(pred.shape[0], pred.shape[1])
        obs = obs.reshape(pred.shape[0], pred.shape[1])
        inds = statError(obs, pred)
        inds_df = pd.DataFrame(inds)

        keys_nse = "NSE"
        xs = []
        ys = []
        cases_exps_legends_together = ["small_stor", "large_stor"]

        x1, y1 = ecdf(inds_df[keys_nse].iloc[idx_lst_nor1])
        xs.append(x1)
        ys.append(y1)

        x2, y2 = ecdf(inds_df[keys_nse].iloc[idx_lst_nor2])
        xs.append(x2)
        ys.append(y2)

        cases_exps = ["dam_exp12", "dam_exp11"]
        cases_exps_legends_separate = ["small_stor", "large_stor"]
        # cases_exps = ["dam_exp4", "dam_exp5", "dam_exp6"]
        # cases_exps = ["dam_exp1", "dam_exp2", "dam_exp3"]
        # cases_exps_legends = ["dam-lstm", "dam-with-natural-flow", "dam-with-kernel"]
        for case_exp in cases_exps:
            config_data_i = load_dataconfig_case_exp(case_exp)
            pred_i, obs_i = load_result(config_data_i.data_path['Temp'], self.test_epoch)
            pred_i = pred_i.reshape(pred_i.shape[0], pred_i.shape[1])
            obs_i = obs_i.reshape(obs_i.shape[0], obs_i.shape[1])
            inds_i = statError(obs_i, pred_i)
            x, y = ecdf(inds_i[keys_nse])
            xs.append(x)
            ys.append(y)

        plot_ecdfs(xs, ys, cases_exps_legends_together + cases_exps_legends_separate,
                   style=["together", "together", "separate", "separate"])


if __name__ == '__main__':
    unittest.main()
